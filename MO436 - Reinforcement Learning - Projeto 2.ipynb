{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MO436 - Tópicos em Aprendizado de Máquina: Reinforcement Learning - 2s2020\n",
    "\n",
    "### Projeto 2\n",
    "Mauricio de Sousa Araujo, 184477 | Raysa Masson Benatti, 176483"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A síntese dos resultados, bem como sua demonstração visual em cada método, podem ser acessados [neste vídeo](google.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo deste trabalho é aplicar métodos de <b>aprendizado por reforço profundo</b> para resolver um problema definido pelo grupo. No nosso caso, optamos por estudar e resolver o problema do <b>CartPole</b>. As configurações do ambiente seguem o mesmo padrão usado no Projeto 1. \n",
    "    \n",
    "Conforme definido por Barto, Sutton e Anderson [[1]](#Referências), trata-se de um sistema simulado que representa um pêndulo invertido. Nessa simulação, o <b>agente</b> é um carrinho que se move em um trilho sem atrito de uma dimensão, para a esquerda ou direita; ligado ao carrinho, encontra-se uma estaca, que se move angularmente, em sentido horário ou anti-horário. O objetivo do problema é colocar o carrinho em movimento de modo que a estaca (pêndulo) permaneça ereta. \n",
    "\n",
    "Para atingir esse objetivo, devemos controlar as variáveis do modelo. São quatro variáveis, que definem os <b>estados</b> do sistema: (1) a posição <b>p</b> do carrinho no trilho; (2) a velocidade <b>v</b> do carrinho; (3) o ângulo <b>a</b> que a estaca forma com o eixo vertical do carrinho; (4) a velocidade angular <b>va</b> de movimento da estaca.\n",
    "\n",
    "O modelo tem duas <b>ações</b> possíveis: movimentar o carrinho sobre o trilho para a esquerda ou para a direita.\n",
    "\n",
    "Um estado desse sistema é <b>terminal</b> nas seguintes situações: (1) se o módulo do ângulo <b>a</b> é maior que 12 graus (o que representa, na prática, a queda da estaca); (2) se o centro do carrinho atinge alguma borda do ambiente; (3) se a quantidade máxima de episódios definida é atingida. O estado terminal (1) é considerado fracasso; os estados terminais (2) e (3) são bem-sucedidos e representam o objetivo do problema <b>se</b>, quando são atingidos, o carrinho se movimenta com a estaca ereta. Por padrão, a cada ação tomada, o ambiente fornece ao agente um <i>feedback</i> --- ou <b>recompensa</b> --- no valor de 1; se a ação resulta em um estado (1) ou (2), o ambiente enviará um <i>failure signal</i> através da variável booleana ``done``, cujo valor será modificado de ``False`` para ``True``. Caso a ação faça a estaca cair (estado (1)), a recompensa será zero.\n",
    "\n",
    "O ambiente se caracteriza como <b>estocástico</b>, pois não é possível determinar o resultado a partir de um estado atual, e <b>episódico</b>, pois o agente pode tomar qualquer uma das ações disponíveis a qualquer momento, sem que uma ação afete as demais. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar e explorar o problema, usamos o ambiente CartPole-v1 do <i>toolkit</i> OpenAI Gym [[2]](#Referências). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foram implementados e comparados dois métodos: <b>DQN</b> (off-policy, value-based) e <b>A2C</b> (on-policy, actor-critic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN (Deep Q-Network) é uma rede neural convolucional treinada para um algoritmo Q-learning [[3]](#Referências). Trata-se de um método off-policy e value-based de aprendizado por reforço profundo.\n",
    "\n",
    "Aqui, implementamos o DQN da seguinte forma: criamos duas redes idênticas, uma denominada predict_net($Q^{predict}_{w}$) e a outra target_net($Q^{target}_{w}$), em que $w$ representa sua parametrização. A entrada da rede é uma estado representado por 4 números (conforme descrito na introdução), e a saída é a expectativa de reward futuro para cada razão. Então, treinamos as redes por um número de $n$ episódios segundo o algoritmo: \n",
    "\n",
    "1.   Selecionar uma ação $a_t$ de acordo com a política $\\epsilon-greedy$ sobre a rede  $Q^{predict}_{w}$;\n",
    "2.   Armazenar a transição $(s_t,\\ a_t, \\ r_{t+1}, \\ s_{t+1})$ em um $\\textit{replay memory D} $;\n",
    "3.   Retirar randomicamente $\\textit{mini-batch}$ de $\\textit{D}$ transições: $(s,a,r,s')$;\n",
    "4.   Computar o $\\textit{target value}$ de uma amostra: $r\\ + \\ \\gamma max_{a'} Q^{target}_w(s',a')$;\n",
    "5.   Otimizar com MSE:  $$Loss = E_{s,a,r,s'\\sim  D}[(r\\ + \\ \\gamma max_{a'} Q^{target}_w(s',a')-Q_w^{prediction}(s,a))^2]$$ (podemos subistituir a expectativa $E$ por uma média ou por um sample utilizando SGD para atualizar os pesos).\n",
    "\n",
    "Vale lembrar que, efetivamente, a rede treinada é $Q^{predict}_{w}$; a rede $Q^{target}_{w}$ é atualizada com os pesos da rede ``predict`` a cada 10 episódios. Ou seja: durante o treinamento, deixamos a rede ``target`` com suas camadas congeladas.\n",
    "\n",
    "Configuração do algoritmo:\n",
    "1. Arquitetura de rede:\n",
    "    1.  $\\textit{Input}$ com tamanho igual a 4; \n",
    "    2.  Após o $\\textit{input}$, duas camadas $\\textit{Dense}$ com 124 neurônios com função de ativação ReLU;\n",
    "    3.  Por fim, uma camada linear de $\\textit{output}$ de tamanho igual a 2, representando a expectativa de retorno para as duas ações possíveis;\n",
    "    4. A $\\textit{loss}$ utilizada foi MSE.\n",
    "2. Constantes:\n",
    "    1. Treinamos sobre 40 eposódios;\n",
    "    2. Nossa $\\textit{replay memory} $ tem tamanho igual a 1000 e retiramos $\\textit{batchs} $ de tamanho igual a 32;\n",
    "    3. $\\gamma = 0.95$;  \n",
    "    4. Atualizamos $Q^{target}_{w}$ a cada 10 épocas.\n",
    "3. Métricas de avaliação:\n",
    "   Para sabermos se modelo está aprendendo a equilibrar o bastão, geramos uma política $\\epsilon-greedy$ com base na função $Q^{target}_{w}$, ou seja, tomamos a ação que gera a maior expectativa de $\\textit{reward}$, dado um estado, e comparamos tal política com a política randômica.\n",
    "   Além disso, plotamos o gráfico de $\\textit{score}$. Esse gráfico representa a quantidade de ações que a política $\\epsilon-greedy$ fez mantendo o bloco entre estados válidos para um único sample de estados iniciais válidos.\n",
    "   \n",
    "Os experimentos foram executados localmente; o código e os resultados são reproduzidos aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constantes\n",
    "batch_size = 32\n",
    "TARGET_UPDATE = 10\n",
    "EPISODES = 40\n",
    "GAMMA = 0.95\n",
    "SIZE_REPLAY = 1000\n",
    "\n",
    "#Data Structure\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = GAMMA    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        \n",
    "        self.memory = ReplayMemory(SIZE_REPLAY)\n",
    "        self.predict_net = self._build_model()\n",
    "        self.target_net = self.disable_training_network( self._build_model() )\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def disable_training_network(self, model):\n",
    "        #Congelamos a rede\n",
    "        for k,v in model._get_trainable_state().items():\n",
    "            k.trainable = False\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "            \n",
    "    def salve_model(self,model, path):\n",
    "        model.save_weights(path)\n",
    "        \n",
    "    def load_model(self, model, path):\n",
    "        model.load_weights(path)\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state, model):\n",
    "        #Epsilon-Greedy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def optimize_model(self, batch_size):\n",
    "        \n",
    "        minibatch = self.memory.sample(batch_size)        \n",
    "        for t in minibatch:\n",
    "            state, action, reward, next_state, done = t.state, t.action, t.reward, t.next_state, t.done\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.target_net.predict(next_state)[0]))\n",
    "            target_f = self.predict_net.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.predict_net.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQN(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segue o treinamento do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for e in range(EPISODES): \n",
    "    state = np.reshape(env.reset(), [1, state_size])    \n",
    "    for time in range(500):\n",
    "        action = agent.act(state, agent.predict_net)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        new_state = np.reshape(new_state, [1, state_size])\n",
    "        \n",
    "        agent.memorize(state, action, new_state, reward,done)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            scores.append(time)\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.optimize_model(batch_size)\n",
    "    if e % TARGET_UPDATE == 0:\n",
    "        agent.salve_model(model=agent.predict_net, path='model_DQN')\n",
    "        agent.load_model(model=agent.target_net, path='model_DQN')\n",
    "\n",
    "#Damos um checkpoint de saída \n",
    "if e % TARGET_UPDATE == 0:\n",
    "    agent.salve_model(model=agent.predict_net, path='model_DQN')\n",
    "    agent.load_model(model=agent.target_net, path='model_DQN')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "episode: 0/40, score: 28, e: 1.0\n",
    "episode: 1/40, score: 32, e: 0.86\n",
    "episode: 2/40, score: 26, e: 0.76\n",
    "episode: 3/40, score: 27, e: 0.66\n",
    "episode: 4/40, score: 12, e: 0.62\n",
    "episode: 5/40, score: 9, e: 0.6\n",
    "episode: 6/40, score: 8, e: 0.57\n",
    "episode: 7/40, score: 13, e: 0.54\n",
    "episode: 8/40, score: 12, e: 0.51\n",
    "episode: 9/40, score: 10, e: 0.48\n",
    "episode: 10/40, score: 10, e: 0.46\n",
    "episode: 11/40, score: 26, e: 0.4\n",
    "episode: 12/40, score: 16, e: 0.37\n",
    "episode: 13/40, score: 24, e: 0.33\n",
    "episode: 14/40, score: 25, e: 0.29\n",
    "episode: 15/40, score: 21, e: 0.26\n",
    "episode: 16/40, score: 13, e: 0.24\n",
    "episode: 17/40, score: 11, e: 0.23\n",
    "episode: 18/40, score: 15, e: 0.21\n",
    "episode: 19/40, score: 18, e: 0.2\n",
    "episode: 20/40, score: 43, e: 0.16\n",
    "episode: 21/40, score: 15, e: 0.15\n",
    "episode: 22/40, score: 14, e: 0.14\n",
    "episode: 23/40, score: 15, e: 0.13\n",
    "episode: 24/40, score: 14, e: 0.12\n",
    "episode: 25/40, score: 17, e: 0.11\n",
    "episode: 26/40, score: 17, e: 0.1\n",
    "episode: 27/40, score: 17, e: 0.092\n",
    "episode: 28/40, score: 27, e: 0.08\n",
    "episode: 29/40, score: 17, e: 0.073\n",
    "episode: 30/40, score: 86, e: 0.048\n",
    "episode: 31/40, score: 176, e: 0.02\n",
    "episode: 32/40, score: 128, e: 0.01\n",
    "episode: 33/40, score: 221, e: 0.01\n",
    "episode: 34/40, score: 144, e: 0.01\n",
    "episode: 35/40, score: 143, e: 0.01\n",
    "episode: 36/40, score: 119, e: 0.01\n",
    "episode: 37/40, score: 122, e: 0.01\n",
    "episode: 38/40, score: 114, e: 0.01\n",
    "episode: 39/40, score: 214, e: 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliamos o modelo observando a evolução do score ao longo do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(scores, label = 'score')\n",
    "plt.legend()\n",
    "plt.title('Score versus Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evolução do score](scores.png \"Evolução do score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, calculamos: (a) a média de ações para política $\\epsilon-greedy$ com base na função $Q^{target}_{w}$; (b) a média de ações para política randômica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "soma=0\n",
    "for i in range(n):\n",
    "    state = np.reshape(env.reset(), [1, state_size])  \n",
    "    k=0  \n",
    "    for time in range(500):\n",
    "        action = agent.act(state, agent.target_net)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_state = np.reshape(new_state, [1, state_size])\n",
    "        state = new_state\n",
    "        k+=1\n",
    "        if done:\n",
    "            soma+=k      \n",
    "            break\n",
    "print(f'Média de ações para {EPISODES} episódios é {soma/n}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Média de ações para 40 episódios é 131.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "soma=0\n",
    "for i in range(n):\n",
    "    state = np.reshape(env.reset(), [1, state_size])  \n",
    "    k=0  \n",
    "    for time in range(500):\n",
    "        action_random = env.action_space.sample()\n",
    "        new_state, reward, done, _ = env.step(action_random)\n",
    "        new_state = np.reshape(new_state, [1, state_size])\n",
    "        state = new_state\n",
    "        k+=1\n",
    "        if done:\n",
    "            soma+=k      \n",
    "            break\n",
    "print(f'Média de ações para política randômica é {soma/n}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Média de ações para política randômica é 21.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Para carregar o modelo salvo:</b> O trecho de código abaixo serve para pegar diretamente os pesos da rede treinada. Para carregar, são necessários os arquivos dos pesos no diretório corrente. Eles podem ser acessados [aqui](https://github.com/ra-ysa/reinforcement_learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer=Adam())\n",
    "\n",
    "model.load_weights('model_DQN')\n",
    "\n",
    "soma=0\n",
    "n=100\n",
    "for i in range(n):\n",
    "    state = np.reshape(env.reset(), [1, state_size])    \n",
    "    done=False\n",
    "    k=0\n",
    "    while(not done):\n",
    "        action = np.argmax(model.predict(state),axis=1)[0]\n",
    "        next_state, reward, done, info = env.step(action) #obs é um vetor com 4 valores, Position, Cart Velocity, Angle, Angle Velocity\n",
    "        next_state = np.reshape(next_state, [1, state_size])  \n",
    "        state = next_state\n",
    "        k+=1\n",
    "    mean+=k\n",
    "\n",
    "print(f'Média de: {soma/n} ações antes de cair')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
    "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
    "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
    "Média de: 0.0 ações antes de cair para 100 episodeios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C é a abreviação de <b>Advantage Actor Critic</b>, uma variação síncrona e determinística do método A3C (Asynchronous Advantage Actor Critic) [[4]](#Referências). Trata-se de métodos <b>actor-critic</b>, isto é, em que a política é o <b>ator</b> (responsável por selecionar ações) e a função valor é o <b>crítico</b> (responsável por \"criticar\" as ações tomadas pelo ator). O aprendizado ocorre on-policy.\n",
    "\n",
    "Usando a biblioteca ``stable_baselines``, implementamos o algoritmo usando os seguintes parâmetros:\n",
    "\n",
    "- 'MlpPolicy' --- isto é, estrutura actor-critic com MLP (multilayer perceptron de 2 camadas de 64 neurônios) como aproximador de função;\n",
    "- Taxa de aprendizado de 0.001; \n",
    "- 25000 steps para treinamento. \n",
    "\n",
    "Foram testados dois valores do fator de desconto (gamma): 0.5 e 0.99. (Quanto maior o valor de gamma, maior é a importância das recompensas distantes.)\n",
    "\n",
    "Os experimentos foram executados localmente; o código e os resultados são reproduzidos aqui. Os logs gerados por cada treinamento, por serem longos, foram omitidos para facilitar a leitura do notebook; [aqui](https://github.com/ra-ysa/reinforcement_learning) se encontra seu conteúdo completo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, gamma, learning_rate):\n",
    "    model = A2C('MlpPolicy', env, learning_rate, verbose=1)\n",
    "    init_time = time.time()\n",
    "    model.learn(total_timesteps=25000, log_interval=50)\n",
    "    final_time = time.time()\n",
    "    model.save(\"a2c_cartpole\")\n",
    "    print(\"Tempo total de treinamento: {} segundos\".format(final_time - init_time))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho abaixo executa a demonstração (visualização) do ambiente, disponível no vídeo referenciado no início do relatório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate(model):\n",
    "    del model # remove to demonstrate saving and loading\n",
    "    model = A2C.load(\"a2c_cartpole\")\n",
    "    obs = env.reset()\n",
    "    for i in range(1000): \n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A avaliação é feita pela evolução dos valores de loss, obtidos a partir do log do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(log, plot_name):\n",
    "    '''\n",
    "    A partir do log de treinamento, plota os valores da loss\n",
    "    '''\n",
    "    timesteps = []\n",
    "    losses = []\n",
    "\n",
    "    file = open(log, 'r')\n",
    "    for line in file:\n",
    "        if (\"total_timesteps\" in line):\n",
    "            timesteps.append(float(str.strip(line[22:32])))\n",
    "        if (\"value_loss\" in line):\n",
    "            losses.append(float(str.strip(line[22:32])))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(timesteps, losses)\n",
    "    ax.set(xlabel='Timesteps', ylabel='Loss',\n",
    "       title='Evolução do valor da loss ao longo do treinamento')\n",
    "    ax.grid()\n",
    "    fig.savefig(plot_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "learning_rate=0.001\n",
    "gamma = 0.5\n",
    "\n",
    "trained_model = train(env, gamma, learning_rate)\n",
    "demonstrate(trained_model)\n",
    "log = \"log1.txt\"\n",
    "evaluate(log, \"loss1.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tempo total de treinamento: 22.369824409484863 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evolução da loss para gamma=0.5](loss1.png \"Evolução da loss para gamma=0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, executamos o mesmo experimento para gamma = 0.99. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "trained_model = train(env, gamma, learning_rate)\n",
    "demonstrate(trained_model)\n",
    "log = \"log2.txt\"\n",
    "evaluate(log, \"loss2.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tempo total de treinamento: 23.775914907455444 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evolução da loss para gamma=0.99](loss2.png \"Evolução da loss para gamma=0.99\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Síntese dos resultados e discussões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> DQN </b>:\n",
    "\n",
    "O modelo apresentou um resultado satisfatório, embora possivelmente prejudicado pela baixa quantidade de episódios de treinamento. Embora não registrado no código, o tempo de treinamento, para 40 episódios, foi de mais de 2 horas, o que inviabilizou a realização de experimentos com treinamento maior. O valor do score salta a partir de 30 episódios, mas ainda apresenta oscilações; possivelmente, com mais episódios veríamos uma tendência de convergência mais clara. Os valores atingidos, porém, são aceitáveis (o problema pode ser considerado solucionado quando o score chega a 195 e se mantém próximo disso). \n",
    "\n",
    "A política greedy se mostrou consideravelmente melhor que a política randômica, com média de 131.14 ações executadas (contra 21.39) - o que demonstra a adequação do modelo para resolver este problema.\n",
    "\n",
    "- <b> A2C </b>:\n",
    "\n",
    "O modelo apresentou um resultado satisfatório, com convergência aproximadamente 2500 passos de treinamento. O tempo de aprendizado foi consideravelmente mais rápido que o do DQN; notamos, porém, que DQN seguiu uma implementação passo a passo, enquanto para A2C foi utilizada uma biblioteca otimizada, o que certamente tem influência nesse resultado.\n",
    "\n",
    "Não houve diferença significativa entre A2C com gamma=0.5 e A2C com gamma=0.99. Experimentos preliminares (não descritos aqui) usando outras configurações de redes MLP para aproximar a função também não mostraram diferenças importantes no resultado. Entendemos isso como indicativo de que: (a) na dinâmica actor-critic, diferenciar o peso das rewards ao longo do tempo parece ter menos relevância que em outros modelos; (b) MLP é uma arquitetura de rede adequada para realizar a tarefa proposta aqui. \n",
    "\n",
    "- <b>Comparação entre algoritmos</b>:\n",
    "\n",
    "Abaixo, comparamos os métodos implementados aqui com os algoritmos implementados e avaliados no Projeto 1 (Monte Carlo, Q-learning e Sarsa-Lambda). \n",
    "\n",
    "|           |# episódios necessários para convergência| Tempo de treinamento|Resultado|\n",
    "|-----------|---------------------------------|----------|----------|\n",
    "|Monte Carlo|aprox. 100|1.43 segundos (1000 episódios)|Satisfatório, com convergência para solução|\n",
    "|Q-learning|aprox. 200|296 segundos (1000 episódios)|Satisfatório, com convergência para solução|\n",
    "|Sarsa-Lambda|N/A|1468 segundos (1000 episódios)|Não houve convergência|\n",
    "|DQN|Não foi possível avaliar|>2 horas (40 episódios)|Parcialmente satisfatório|\n",
    "|A2C|aprox. 2500|~23 segundos (25000 episódios)|Satisfatório, com convergência para solução|\n",
    "\n",
    "Embora o DQN não tenha exibido um padrão claro de convergência, a avaliação do modelo sugere que ele teria convergido com mais episódios para treinamento. Apenas Sarsa-Lambda exibiu um resultado inferior - já que, com o treinamento disponível, não exibiu nenhum padrão de aprendizado.\n",
    "\n",
    "Em todos os demais métodos, o objetivo de aprendizado foi atingido. Em particular, os aproximadores de função usados aqui têm como vantagem a dispensa da necessidade de discretizar o ambiente. No caso da formulação A2C, apesar de demandar mais episódios que Monte Carlo e Q-learning para convergir, seu treinamento rápido, aliado à possibilidade de execução sem discretização, representou uma vantagem.\n",
    "\n",
    "Neste problema, o aprendizado off-policy se revelou menos eficiente que on-policy; não se deve perder de vista, porém, que o uso de uma biblioteca otimizada para o último pode ter influenciado este resultado.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descrição das tarefas do grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As tarefas foram realizadas conforme descrito abaixo: \n",
    "\n",
    "- Mauricio: Estudo, implementação, testes e visualizações do método DQN; Escrita e organização de partes do relatório; Produção do vídeo.\n",
    "- Raysa: Estudo, implementação, testes e visualizações do método A2C; Escrita e organização de partes do relatório; Produção do vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] A. G. Barto, R. S. Sutton and C. W. Anderson, \"Neuronlike adaptive elements that can solve difficult learning control problems,\" in <i>IEEE Transactions on Systems, Man, and Cybernetics</i>, vol. SMC-13, no. 5, pp. 834-846, Sept.-Oct. 1983, doi: 10.1109/TSMC.1983.6313077.\n",
    "\n",
    "[2] OpenAI Gym: CartPole-v0. https://gym.openai.com/envs/CartPole-v0/ \n",
    "\n",
    "[3] V. Mnih et al. Playing Atari with Deep Reinforcement Learning. 2013. https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "[4] OpenAI Baselines: ACKTR & A2C. https://openai.com/blog/baselines-acktr-a2c/. Documentação em https://stable-baselines.readthedocs.io/en/master/modules/a2c.html. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
